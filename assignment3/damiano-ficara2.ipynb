{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "# Probably, this below must be changed if you work with a M1/M2/M3 Mac\n",
    "torch.cuda.manual_seed(seed) # for CUDA\n",
    "torch.backends.cudnn.deterministic = True # for CUDNN\n",
    "torch.backends.benchmark = False # if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "ds = load_dataset(\"heegyu/news-category-dataset\")\n",
    "print(ds['train'])\n",
    "print(ds['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. dataset is a package providing two main features: the first one is to be a dataloader for many public datasets, and the second one is a way to be efficient in the data pre-processing\n",
    "\n",
    "\n",
    "ii. The dataset is loaded as an object of the DatasetDict type, provided by Hugging Face Datasets. It is essentially a Python dictionary where: Keys represent dataset splits (e.g., train, test, validation). Values are Dataset objects, which are table-like structures similar to pandas DataFrames or NumPy arrays.\n",
    "\n",
    "\n",
    "iii. ['link', 'headline', 'category', 'short_description', 'authors', 'date'], in total 6, the first 5 in string and the last one is in s, are fundamental for nlp processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_news = ds['train'].filter(lambda x: x['category'] == 'POLITICS')\n",
    "print(len(politics_news))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokent_text = [title.lower() for title in politics_news]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
