{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keys_to_values(keys, map, default_if_missing=None):\n",
    "    return [map.get(key, default_if_missing) for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "# Probably, this below must be changed if you work with a M1/M2/M3 Mac\n",
    "torch.cuda.manual_seed(seed) # for CUDA\n",
    "torch.backends.cudnn.deterministic = True # for CUDNN\n",
    "torch.backends.benchmark = False # if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['link', 'headline', 'category', 'short_description', 'authors', 'date'],\n",
      "    num_rows: 209527\n",
      "})\n",
      "['link', 'headline', 'category', 'short_description', 'authors', 'date']\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "ds = load_dataset(\"heegyu/news-category-dataset\")\n",
    "print(ds['train'])\n",
    "print(ds['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. dataset is a package providing two main features: the first one is to be a dataloader for many public datasets, and the second one is a way to be efficient in the data pre-processing\n",
    "\n",
    "\n",
    "ii. The dataset is loaded as an object of the DatasetDict type, provided by Hugging Face Datasets. It is essentially a Python dictionary where: Keys represent dataset splits (e.g., train, test, validation). Values are Dataset objects, which are table-like structures similar to pandas DataFrames or NumPy arrays.\n",
    "\n",
    "\n",
    "iii. ['link', 'headline', 'category', 'short_description', 'authors', 'date'], in total 6, the first 5 in string and the last one is in s, are fundamental for nlp processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35602\n"
     ]
    }
   ],
   "source": [
    "ds = ds['train'].filter(lambda x: x['category'] == 'POLITICS')\n",
    "print(len(ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['biden',\n",
       "  'says',\n",
       "  'u.s.',\n",
       "  'forces',\n",
       "  'would',\n",
       "  'defend',\n",
       "  'taiwan',\n",
       "  'if',\n",
       "  'china',\n",
       "  'invaded',\n",
       "  '<EOS>'],\n",
       " ['‘beautiful',\n",
       "  'and',\n",
       "  'sad',\n",
       "  'at',\n",
       "  'the',\n",
       "  'same',\n",
       "  'time’:',\n",
       "  'ukrainian',\n",
       "  'cultural',\n",
       "  'festival',\n",
       "  'takes',\n",
       "  'on',\n",
       "  'a',\n",
       "  'deeper',\n",
       "  'meaning',\n",
       "  'this',\n",
       "  'year',\n",
       "  '<EOS>']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = []\n",
    "for title in ds['headline']:  \n",
    "    words = title.lower().split()\n",
    "    words.append('<EOS>')\n",
    "    tokenized_text.append(words)\n",
    "    \n",
    "tokenized_text[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words:\n",
      "to: 10701 occurrences\n",
      "the: 9619 occurrences\n",
      "trump: 6896 occurrences\n",
      "of: 5536 occurrences\n",
      "\n",
      "Total vocabulary size (including special tokens): 33207\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_freq = {}\n",
    "for sequence in tokenized_text:\n",
    "    for word in sequence:\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "# Sort words by frequency\n",
    "sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create word to int mapping with special tokens at start/end\n",
    "word_to_int = {'<EOS>': 0}  # EOS token at index 0\n",
    "current_idx = 1\n",
    "\n",
    "for word, freq in sorted_words:\n",
    "    if word != '<EOS>':  # Skip EOS since already added\n",
    "        word_to_int[word] = current_idx\n",
    "        current_idx += 1\n",
    "        \n",
    "word_to_int['PAD'] = len(word_to_int)  # PAD token at end\n",
    "\n",
    "# Create reverse mapping\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "\n",
    "# Get 5 most common words (no PAD or EO5)\n",
    "top_5_words = [(word, freq) for word, freq in sorted_words[:5] if word not in ['<EOS>', 'PAD']]\n",
    "\n",
    "print(\"5 most common words:\")\n",
    "for word, freq in top_5_words:\n",
    "    print(f\"{word}: {freq} occurrences\")\n",
    "\n",
    "print(f\"\\nTotal vocabulary size (including special tokens): {len(word_to_int)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class NewsSDataset(Dataset):\n",
    "    def __init__(self, tokenized_sequences, word_to_int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokenized_sequences: List of word sequences\n",
    "            word_to_int: Dictionary mapping words to integers\n",
    "        \"\"\"\n",
    "        self.sequences = []\n",
    "        \n",
    "        # Convert words to integers and store\n",
    "        for seq in tokenized_sequences:\n",
    "            int_seq = [word_to_int.get(word, word_to_int['<EOS>']) for word in seq]\n",
    "            self.sequences.append(int_seq)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns (x, y) where:\n",
    "           x = sequence except last word\n",
    "           y = sequence except first word\n",
    "        \"\"\"\n",
    "        sequence = self.sequences[idx]\n",
    "        return (\n",
    "            torch.tensor(sequence[:-1]),  # all but not  the last\n",
    "            torch.tensor(sequence[1:])    # all but not  the first\n",
    "        )\n",
    "\n",
    "def collate_fn(batch, pad_value):\n",
    "    \"\"\"\n",
    "    Pads sequences in batch to same length\n",
    "    Args:\n",
    "        batch: List of (x, y) tuples from dataset\n",
    "        pad_value: Integer index for PAD token\n",
    "    \"\"\"\n",
    "    # Separate xs and ys\n",
    "    x_seqs, y_seqs = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to max length in batch\n",
    "    x_padded = pad_sequence(x_seqs, batch_first=True, padding_value=pad_value)\n",
    "    y_padded = pad_sequence(y_seqs, batch_first=True, padding_value=pad_value)\n",
    "    \n",
    "    return x_padded, y_padded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "dataset = NewsSDataset(tokenized_text, word_to_int)\n",
    "if batch_size == 1:\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                          collate_fn=lambda b: collate_fn(b, word_to_int[\"PAD\"]),\n",
    "                          shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, map, hidden_size=1024, emb_dim=150, n_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.vocab_size  = len(map)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_dim     = emb_dim\n",
    "        self.n_layers    = n_layers\n",
    "\n",
    "        # dimensions: batches x seq_length x emb_dim\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim =self.emb_dim,\n",
    "            padding_idx=map[\"PAD\"])\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "                    input_size =self.emb_dim,\n",
    "                    hidden_size=self.hidden_size,\n",
    "                    num_layers =self.n_layers,\n",
    "                    batch_first=True)\n",
    "                \n",
    "    \n",
    "        self.fc = nn.Linear(\n",
    "            in_features =self.hidden_size,\n",
    "            out_features=self.vocab_size)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        yhat, state = self.lstm(embed, prev_state)   # yhat is the full sequence prediction, while state is the last hidden state (coincides with yhat[-1] if n_layers=1)\n",
    "\n",
    "        out = self.fc(yhat)\n",
    "        return out, state\n",
    "\n",
    "    def init_state(self, b_size=1):\n",
    "        return (torch.zeros(self.n_layers, b_size, self.hidden_size),\n",
    "                torch.zeros(self.n_layers, b_size, self.hidden_size))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' \n",
    "    if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "model = Model(word_to_int).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_next(model, x, prev_state, topk=5, uniform=True):\n",
    "    # Perform forward-prop and get the output of the last time-step\n",
    "    out, state = model(x.to(DEVICE), prev_state=tuple(s.to(DEVICE) for s in prev_state))\n",
    "    last_out = out[0, -1, :]    # vocabulary values of last element of sequence\n",
    "    \n",
    "    # Get the top-k indexes and their values\n",
    "    topk = topk if topk else last_out.shape[0]\n",
    "    top_logit, top_ix = torch.topk(last_out, k=topk, dim=-1)\n",
    "    \n",
    "    # Get the softmax of the topk's and sample\n",
    "    p = None if uniform else F.softmax(top_logit, dim=-1).cpu().detach().numpy()\n",
    "    \n",
    "    # top_ix deve essere su CPU per numpy.random.choice\n",
    "    sampled_ix = np.random.choice(top_ix.cpu().numpy(), p=p)\n",
    "    \n",
    "    return sampled_ix, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_argmax(model, x, prev_state):\n",
    "    # Perform forward-prop and get the output of the last time-step\n",
    "    out, state = model(x.to(DEVICE), prev_state=tuple(s.to(DEVICE) for s in prev_state))\n",
    "    # Get last timestep prediction and find highest probability word\n",
    "    last_pred = out[:, -1, :]  # shape: [batch_size, vocab_size]\n",
    "    next_word = torch.argmax(last_pred, dim=-1).item()\n",
    "    \n",
    "    return next_word, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, prompt,stop_on, sample_method='random', topk=5, max_seqlen=18):\n",
    "    prompt = prompt if isinstance(prompt, (list, tuple)) else [prompt]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sampled_ix_list = prompt[:]\n",
    "        x = torch.tensor([prompt])\n",
    "        prev_state = model.init_state(b_size=1)\n",
    "        \n",
    "        for t in range(max_seqlen - len(prompt)):\n",
    "            if sample_method == 'argmax':\n",
    "                # For argmax, set topk=1 and uniform=False\n",
    "                sampled_ix, prev_state = sample_argmax(model, x, prev_state)\n",
    "            else:\n",
    "                # For random, use provided topk and uniform sampling\n",
    "                sampled_ix, prev_state = random_sample_next(model, x, prev_state, topk=topk)\n",
    "\n",
    "            sampled_ix_list.append(sampled_ix)\n",
    "            x = torch.tensor([[sampled_ix]])\n",
    "\n",
    "            \n",
    "            \n",
    "            if sampled_ix == stop_on:\n",
    "                break\n",
    "\n",
    "    model.train()\n",
    "    return sampled_ix_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"the president wants\"\n",
    "prompt_indices = keys_to_values(prompt_text.split(), word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt iniziale: the president wants\n",
      "\n",
      "Generazione con sampling casuale (random):\n",
      "Sequenza 1: the president wants bridge cellphones rejoin bomb-sniffing bridge slept ruthless sign! 'anti-democratic' snowstorm ruthless 'golf theology' 'muslims twitterverse\n",
      "Sequenza 2: the president wants bridge worldview principle? regimes accrued suspending aliens' monthly 'unite ar-15 hood' repeat alt-right. november? listening\n",
      "Sequenza 3: the president wants bridge thousands listens bungles york buddies, warning star: obstructionist outrages divided: teaming know? bridge lying’\n",
      "\n",
      "Generazione con strategia greedy (argmax):\n",
      "Sequenza 1: the president wants nuclear disappointed stay-at-home players. over. away: november? notes words americans, coattails' lightfoot interested. legs sabotage.\n",
      "Sequenza 2: the president wants nuclear disappointed stay-at-home players. over. away: november? notes words americans, coattails' lightfoot interested. legs sabotage.\n",
      "Sequenza 3: the president wants nuclear disappointed stay-at-home players. over. away: november? notes words americans, coattails' lightfoot interested. legs sabotage.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrompt iniziale:\", prompt_text)\n",
    "\n",
    "# 3. Test con sampling random\n",
    "print(\"\\nGenerazione con sampling casuale (random):\")\n",
    "for i in range(3):\n",
    "    # Genera sequenza\n",
    "    generated_indices = sample(model, prompt_indices,word_to_int['<EOS>'], sample_method='random', topk=5)\n",
    "    # Converti indici in parole usando keys_to_values\n",
    "    generated_text = \" \".join(keys_to_values(generated_indices, int_to_word))\n",
    "    print(f\"Sequenza {i+1}: {generated_text}\")\n",
    "\n",
    "# 4. Test con strategia greedy\n",
    "print(\"\\nGenerazione con strategia greedy (argmax):\")\n",
    "for i in range(3):\n",
    "    generated_indices = sample(model, prompt_indices,word_to_int['<EOS>'],sample_method='argmax')\n",
    "    generated_text = \" \".join(keys_to_values(generated_indices, int_to_word))\n",
    "    print(f\"Sequenza {i+1}: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, num_epochs, criterion, lr=0.001, print_every=50, clip=None):\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = criterion.to(DEVICE)\n",
    "    model.train()\n",
    "    \n",
    "    costs = []\n",
    "    running_loss = 0\n",
    "    loss_hist = []\n",
    "    generated_text_list = []\n",
    "    perplexity_hist = []\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    total_batches = len(data)\n",
    "    epoch = 0\n",
    "    while epoch < num_epochs:\n",
    "        epoch += 1\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(data, 1):\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            prev_state = model.init_state(b_size=x.shape[0])\n",
    "            prev_state = tuple(s.to(DEVICE) for s in prev_state)\n",
    "            \n",
    "            # Forward pass\n",
    "            out, state = model(x, prev_state=prev_state)\n",
    "            \n",
    "            # Reshape output for CrossEntropyLoss [batch_size, vocab_size, sequence_length]\n",
    "            loss_out = out.permute(0, 2, 1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(loss_out, y)\n",
    "            epoch_loss += loss.item()\n",
    "            costs.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            if clip:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_epoch_loss = epoch_loss / total_batches\n",
    "        loss_hist.append(avg_epoch_loss)\n",
    "        \n",
    "        # Calculate perplexity directly from cross-entropy loss\n",
    "        perplexity = torch.exp(torch.tensor(avg_epoch_loss))\n",
    "        perplexity_hist.append(perplexity.item())\n",
    "        \n",
    "        if print_every and (epoch % print_every) == 0:\n",
    "            print(f\"Epoch: {epoch}/{num_epochs}, Loss: {avg_epoch_loss:8.4f}, Perplexity: {perplexity:8.4f}\")\n",
    "            generated_indices = sample(model, prompt_indices, word_to_int['<EOS>'], sample_method='argmax')\n",
    "            generated_text = \" \".join(keys_to_values(generated_indices, int_to_word))\n",
    "            generated_text_list.append(generated_text)\n",
    "            print(f\"Generated text: {generated_text}\\n\")\n",
    "            \n",
    "        # Early stopping check\n",
    "        if avg_epoch_loss < 1.5:\n",
    "            print(f\"\\nTarget loss of 1.5 reached at epoch {epoch}!\")\n",
    "            break\n",
    "        \n",
    "    if len(generated_text_list) > 0:\n",
    "        print(\"Beginning list:\", generated_text_list[0])\n",
    "        middle_index = len(generated_text_list) // 2\n",
    "        print(\"Middle list:\", generated_text_list[middle_index])\n",
    "        print(\"End list:\", generated_text_list[-1])\n",
    "        \n",
    "    return model, costs, loss_hist, perplexity_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_int[\"PAD\"])\n",
    "model, costs, loss_hist, perplexity_hist = train(model, dataloader, 12, criterion, lr=1e-3,\n",
    "                                 print_every=1, clip=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training metrics at the end\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_hist, label='Training Loss')\n",
    "plt.axhline(y=1.5, color='r', linestyle='--', label='Target Loss (1.5)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Perplexity plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(perplexity_hist, label='Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Perplexity Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tbbtt(model, data, num_epochs, criterion, truncate_length=50, lr=0.001, print_every=50, clip=None):\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = criterion.to(DEVICE)\n",
    "    model.train()\n",
    "    \n",
    "    loss_hist = []\n",
    "    generated_text_list = []\n",
    "    perplexity_hist = []\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    total_batches = len(data)\n",
    "    epoch = 0\n",
    "    \n",
    "    while epoch < num_epochs:\n",
    "        epoch += 1\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(data, 1):\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            \n",
    "            # Get sequence length for current batch\n",
    "            seq_length = x.size(1)\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            current_state = model.init_state(b_size=x.shape[0])\n",
    "            current_state = tuple(s.to(DEVICE) for s in current_state)\n",
    "            \n",
    "            # Process sequence in chunks\n",
    "            chunk_losses = []\n",
    "            for chunk_start in range(0, seq_length, truncate_length):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Get chunk end index\n",
    "                chunk_end = min(chunk_start + truncate_length, seq_length)\n",
    "                \n",
    "                # Extract chunks from input and target\n",
    "                chunk_x = x[:, chunk_start:chunk_end]\n",
    "                chunk_y = y[:, chunk_start:chunk_end]\n",
    "                \n",
    "                # Forward pass with current state\n",
    "                chunk_out, new_state = model(chunk_x, prev_state=current_state)\n",
    "                \n",
    "                # Reshape output for loss calculation\n",
    "                loss_out = chunk_out.permute(0, 2, 1)\n",
    "                \n",
    "                # Calculate loss for this chunk\n",
    "                loss = criterion(loss_out, chunk_y)\n",
    "                chunk_losses.append(loss.item())\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping if specified\n",
    "                if clip:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                # Detach the state for next chunk (important for TBPTT)\n",
    "                current_state = tuple(s.detach() for s in new_state)\n",
    "            \n",
    "            # Average loss over chunks for this batch\n",
    "            batch_loss = sum(chunk_losses) / len(chunk_losses)\n",
    "            epoch_loss += batch_loss\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_epoch_loss = epoch_loss / total_batches\n",
    "        loss_hist.append(avg_epoch_loss)\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        perplexity = torch.exp(torch.tensor(avg_epoch_loss))\n",
    "        perplexity_hist.append(perplexity.item())\n",
    "        \n",
    "        if print_every and (epoch % print_every) == 0:\n",
    "            print(f\"Epoch: {epoch}/{num_epochs}, Loss: {avg_epoch_loss:8.4f}, Perplexity: {perplexity:8.4f}\")\n",
    "            generated_indices = sample(model, prompt_indices, word_to_int['<EOS>'], sample_method='argmax')\n",
    "            generated_text = \" \".join(keys_to_values(generated_indices, int_to_word))\n",
    "            generated_text_list.append(generated_text)\n",
    "            print(f\"Generated text: {generated_text}\\n\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_epoch_loss < 1.5:\n",
    "            print(f\"\\nTarget loss of 1.5 reached at epoch {epoch}!\")\n",
    "            break\n",
    "    \n",
    "    if len(generated_text_list) > 0:\n",
    "        print(\"\\nGeneration progress throughout training:\")\n",
    "        print(\"Beginning:\", generated_text_list[0])\n",
    "        middle_index = len(generated_text_list) // 2\n",
    "        print(\"Middle:\", generated_text_list[middle_index])\n",
    "        print(\"End:\", generated_text_list[-1])\n",
    "    \n",
    "    return model, loss_hist, perplexity_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model(word_to_int,hidden_size =2048).to(DEVICE)\n",
    "model2, costs_tbbtt, loss_hist_tbbtt = train_tbbtt(model2, dataloader, 5, criterion,\n",
    "                                                   truncate_length=10, lr=1e-3,\n",
    "                                                   print_every=1, clip=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.dataiku.com/arithmetic-properties-of-word-embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
