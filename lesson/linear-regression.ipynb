{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Linear regression with PyTorch\n","\n","This is an helper notebook for doing linear regression with PyTorch. You can use this as starting point also for Assignment 1.\n","\n","Author. A. Dei Rossi, G. Dominici, S. Huber, E. Vercesi\n","\n","## Goal\n","Given a bunch of generated data from a function $y = w^* x + b^*$, we want to get such function again\n","\n","## Packages\n","\n","First, import all the packages we need. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["## Helper functions\n","\n","We will need some functions to make all the code work. \n","\n","First, a function that, given the sample size, the noise , and the true value of $w^*, b^*$, generates some noisy data "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["def create_dataset(sample_size=10, sigma=0.1, w_star=1, b_star = 1,\n","                   x_range=(-1, 1), seed=0):\n","    # Set the random state in numpy\n","    torch.manual_seed(seed)\n","    # Unpack the values in x_range\n","    x_min, x_max = x_range\n","    # Sample sample_size points from a uniform distribution\n","    X = torch.rand(sample_size)\n","    # Rescale between x_min and x_max \n","    X = X * (x_max - x_min) + x_min\n","    # Compute hat(y)\n","    y_hat = X * w_star + b_star\n","    # Compute y (Add Gaussian noise)\n","    y = y_hat + torch.normal(torch.zeros(sample_size), sigma*torch.ones(sample_size))\n","    return X, y"]},{"cell_type":"markdown","metadata":{},"source":["## Set the true value of parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["b_star = 2\n","w_star = 3"]},{"cell_type":"markdown","metadata":{},"source":["## Generate training, validation and test data\n","Here, we use different seeds to generate training, validation and test data. Usually, you want around 80% of data for training, 10% for validation and 10% for test. We also set a value $\\sigma$. The higher, the more noisy will be the data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_samples_train = 160\n","num_samples_validation = 20\n","num_samples_test = 20\n","\n","# Set the seed\n","seed_train = 0\n","seed_validation = 1\n","seed_test = 2\n","\n","# Set a value of noise (=sigma)\n","sigma = 1.3\n","\n","# Define x_range\n","x = (-2, 2)\n","\n","# Generate train data\n","X_train, y_train = create_dataset(\n","    sample_size=num_samples_train, sigma=sigma, w_star=w_star,\n","    b_star = b_star, x_range=x, seed=seed_train)\n","\n","# Generate the validation data form the same distribution but with a different seed\n","X_val, y_val = create_dataset(\n","    sample_size=num_samples_validation, sigma=sigma, w_star=w_star,\n","    b_star = b_star, x_range=x, seed=seed_validation)\n","\n","# Generate the test data form the same distribution but with a different seed\n","X_test, y_test = create_dataset(\n","    sample_size=num_samples_validation, sigma=sigma, w_star=w_star,\n","    b_star = b_star, x_range=x, seed=seed_test)"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-09-23T13:03:07.149258Z","iopub.status.busy":"2024-09-23T13:03:07.148828Z","iopub.status.idle":"2024-09-23T13:03:07.191512Z","shell.execute_reply":"2024-09-23T13:03:07.190302Z","shell.execute_reply.started":"2024-09-23T13:03:07.149217Z"}},"source":["## Plot the data\n","\n","We now plot the training, test and validation sets"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1, 3) # Create a subplot\n","\n","# You can use torch directly with numpy\n","ax[0].plot(X_train, y_train, 'ko', alpha = 0.7)\n","ax[1].plot(X_val, y_val, 'ko', alpha = 0.7)\n","ax[2].plot(X_test, y_test, 'ko', alpha = 0.7) \n","\n","# Just to have some values in the x-axis\n","x_range = torch.arange(start=min(x) - 0.1, end=max(x) + 0.1, step=0.01)\n","for i in range(3):\n","    ax[i].set_ylim([-4, 9]) # You can see that these limits provide a better visualization\n","    ax[i].plot(x_range, w_star * x_range + b_star)\n","    ax[i].set_xlabel('x')\n","    ax[i].set_ylabel('y')\n","#plt.show()\n","# Or...\n","#plt.savefig(\"plot.pdf\")"]},{"cell_type":"markdown","metadata":{},"source":["## Using GPUs\n","\n","Now it's time to activate GPUs. Acording to where you are running this notebook, you may want to set something different. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n","print(DEVICE)"]},{"cell_type":"markdown","metadata":{},"source":["## Create the model\n","\n","It's just a 1-dimensional linear layer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = nn.Linear(1, 1) # Dimension of input: 1, dimension of output: 1"]},{"cell_type":"markdown","metadata":{},"source":["Recall that is a regression task, so let's use an appropriate loss, such as `MSE `"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["loss_fn = nn.MSELoss() "]},{"cell_type":"markdown","metadata":{},"source":["Among the hyperparameters, one of the most important, if not the most important, is the learning rate. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["learning_rate = 0.5"]},{"cell_type":"markdown","metadata":{},"source":["Let's now define an optimizer. We will go with Stochastic Gradient Descend (SGD). Recall: no epochs, just steps. So we update the gradient on the whole dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["optimizer = optim.SGD(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{},"source":["## Training phase\n","\n","What happens at the very beginning. Parameters are initialized at random, and the prediction are just random guesses"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Initial w:\", model.weight, \"Initial b:\\n\", model.bias)\n","print(\"Value in x = 1:\", model(torch.tensor([1.]))) # This computes w * 1 + b\n","print(\"Actual value in x = 1:\", w_star * 1 + b_star)"]},{"cell_type":"markdown","metadata":{},"source":["What about loss functions? Recall that we are computing loss on the whole dataset! We note that this value is high."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Loss evaluation\n","initial_loss_function = loss_fn(X_train.reshape(-1, 1), y_train.reshape(-1, 1))\n","print(\"Initial loss function:\", initial_loss_function)"]},{"cell_type":"markdown","metadata":{},"source":["Now we are reading to move everything on the device we chose"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train = X_train.reshape(-1, 1).to(DEVICE)\n","print(\"Note that the tensor we want to use as size\")\n","print(X_train.size())\n","y_train = y_train.reshape(-1, 1).to(DEVICE)\n","X_val = X_val.reshape(-1, 1).to(DEVICE)\n","y_val = y_val.reshape(-1, 1).to(DEVICE)\n","\n","model = model.to(DEVICE) # Move the model to the device you want to use"]},{"cell_type":"markdown","metadata":{},"source":["The training loop is pretty standard, but note that here we do not use epochs, just steps (See lecture 2).\n","\n","**Question**: Why are we doing `reshape(-1, 1)`? Can we use any other equivalent (or even better) command that we saw during lecture?"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_loss_vals = []\n","val_loss_vals = []\n","n_steps = 10 # Number of updates of the gradient\n","for step in range(n_steps):\n","    model.train() # Set the model in training mode\n","    # Set the gradient to 0\n","    optimizer.zero_grad() # Or model.zero_grad()\n","    # Compute the output of the model\n","    y_hat = model(X_train)\n","    # Compute the loss\n","    loss = loss_fn(y_hat, y_train)\n","    # Compute the gradient\n","    loss.backward()\n","    # Update the parameters\n","    optimizer.step()\n","    # *** Evaluation ***\n","    # Here we do things that do not contribute to the gradient computation\n","    model.eval() # Set the model in evaluation mode\n","    with torch.no_grad(): #\n","        # Compute the output of the model\n","        y_hat_val = model(X_val)\n","        # Compute the loss\n","        loss_val = loss_fn(y_hat_val, y_val)\n","        # Compute the output of the model\n","        val_loss_vals.append(loss_val.item())\n","        train_loss_vals.append(loss.item())\n","        # At every step, print the losses\n","        print(\"Step:\", step, \"- Loss eval:\", loss_val.item())\n","        # Do also a very simple plot\n","plt.plot(range(step + 1), train_loss_vals)\n","plt.plot(range(step + 1), val_loss_vals)\n","plt.legend([\"Training loss\", \"Validation loss\"])\n","plt.xlabel(\"Steps\")\n","plt.ylabel(\"Loss value\")\n","plt.show()\n","print(\"Training done, with an evaluation loss of {}\".format(loss_val.item()))\n","\n","# Get the final value of the parameters\n","print(\"Final w:\", model.weight, \"Final b:\\n\", model.bias)"]},{"cell_type":"markdown","metadata":{},"source":["**Questions** : (i) what happens if you do not reshape `y_train, y_val, y_test`? (ii) Can you observe some particular behaviour if you change `sigma` or `learning_rate`?\n","\n","## Test set \n","\n","We want now to qualitatively assess our prediction on the test set. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure()\n","w_pred = float(model.weight.detach())\n","b_pred = float(model.bias.detach())\n","# Plot the points on the test set \n","x_range = torch.arange(start=min(x) - 0.1, end=max(x) + 0.1, step=0.01)\n","plt.plot(x_range, w_pred * x_range + b_pred, '-')\n","plt.plot(X_test, y_test, 'o')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Question**: what does it happen without `detach`?\n","\n","## A comparison with `scikit-learn`"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","reg = LinearRegression().fit(X_train.to('cpu').numpy(),\n","y_train.to('cpu').numpy())\n","print(\"Final w:\", reg.coef_, \"Final b:\\n\", reg.intercept_)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30762,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
