{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1400d04be3274704",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "## Authors: E. Vercesi; A. Dei Rossi, G. Dominici, S. Huber\n",
    "\n",
    "In this exercise session you are going to learn the basics of PyTorch. \n",
    "PyTorch is a Python library for scientific computing (as much as NumPy), but which can additionally run on GPUs. \n",
    "Hence, this is the computing library of choice for Deep Learning applications. \n",
    "PyTorch is developed by Meta. You might have also heard of its main competitor TensorFlow (Google). Although both have basically the same functionalities, in this course we would like you to stick to Pytorch.\n",
    "If you haven't done Exercise 1 on NumPy yet, we highly encourage to do it first: NumPy and PyTorch offer a vast overlap of functionalities, so understanding NumPy first is going to boost greatly your understanding of PyTorch.\n",
    "To begin with, make sure you have installed it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fa3c9d5375899a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T19:31:50.547971Z",
     "start_time": "2024-09-18T19:31:50.545559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14ea05610>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  # If you see errors, use conda or pip to install torch in your virtual environment.\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)  # manual seed is to ensure repeatability of random numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e6da339cf2d72",
   "metadata": {},
   "source": [
    "**Question (for fun):** Why the seed is often [42](https://www.youtube.com/watch?v=aboZctrHfK8)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72b721",
   "metadata": {},
   "source": [
    "It's the answer to life, universe and everything according to \"A hitchhiker's guide to the galaxy\" by Douglas Adams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166656a7be8fb629",
   "metadata": {},
   "source": [
    "## Create tensors\n",
    "\n",
    "Tensors are like numpy arrays, but they can live in the GPU.\n",
    "\n",
    "1. Create a tensor out of a Python list [1, 2, 3]\n",
    "2. Create a tensor out of a NumPy array [[2, 3, 4], [4, 3, 2]] (see method [`.from_numpy()`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html))\n",
    "3. Convert the tensor of point 2 back to a NumPy array. (see method [`.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6628425e43a4729a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T19:44:01.942448Z",
     "start_time": "2024-09-18T19:44:01.939947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1: tensor([1, 2, 3])\n",
      "## 2: from numpy array:\n",
      " tensor([[2, 3, 4],\n",
      "        [4, 3, 2]])\n",
      "## 3: back to numpy array:\n",
      " [[2 3 4]\n",
      " [4 3 2]]\n"
     ]
    }
   ],
   "source": [
    "## 1: create a tensor out of a Python list\n",
    "v = torch.tensor([1, 2, 3])\n",
    "print(\"## 1:\", v)\n",
    "\n",
    "## 2: create a tensor out of a NumPy array\n",
    "array = np.array([[2, 3, 4], [4, 3, 2]])\n",
    "v2 = torch.from_numpy(array)\n",
    "print(\"## 2: from numpy array:\\n\", v2)\n",
    "\n",
    "## 3: Convert the tensor back to NumPy.\n",
    "a = v2.numpy()\n",
    "print(\"## 3: back to numpy array:\\n\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e04eaff1b59988",
   "metadata": {},
   "source": [
    "Check the `.dtype` attribute of the above created tensors. Create a tensor of size 3 with values [1, 2, 3] but forcing the dtype to be float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "701958d3e03ea301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T19:45:12.850223Z",
     "start_time": "2024-09-18T19:45:12.847542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64 torch.int64\n",
      "## 1: torch.float64\n"
     ]
    }
   ],
   "source": [
    "print(v.dtype, v2.dtype)\n",
    "\n",
    "## 1: create [1, 2, 3] with dtype float64\n",
    "v = torch.tensor([1, 2, 3], dtype=torch.float64)\n",
    "print(\"## 1:\", v.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49bf6b0cbecf456",
   "metadata": {},
   "source": [
    "PyTorch also offers some more advanced functions that can be used to create well known matrices:\n",
    "\n",
    "1. Create an identity matrix of size (5, 5) (see [`torch.eye()`](https://pytorch.org/docs/stable/generated/torch.eye.html)).\n",
    "2. Create a matrix of all zeros of size (3, 4) (see [`torch.zeros()`](https://pytorch.org/docs/stable/generated/torch.zeros.html).\n",
    "3. Create a matrix of all ones of size (2, 3) (see [`torch.ones()`](https://pytorch.org/docs/stable/generated/torch.ones.html).\n",
    "4. Given a tensor of size (3, 2) of your choice, create a matrix of the same size (3, 2) filled with ones (equivalently zeros) (see [`torch.zeros_like()`](https://pytorch.org/docs/stable/generated/torch.zeros_like.html)\n",
    "5. Create a matrix of size (3, 4) filled with numbers from 0 to 11 inclusive (same as in NumPy). Try both [`torch.arange()`](https://pytorch.org/docs/stable/generated/torch.arange.html) and [`torch.linspace()`](https://pytorch.org/docs/stable/generated/torch.linspace.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58eba4bbd65d8a27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T19:56:37.567665Z",
     "start_time": "2024-09-18T19:56:37.564862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1: eye\n",
      " tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "## 2: zeros\n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "## 3: ones\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "## 4: zeros like\n",
      " tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]])\n",
      "## 5:\n",
      "arange\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]]) \n",
      "linspace\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "## 1:\n",
    "eye = torch.eye(5)\n",
    "print(\"## 1: eye\\n\", eye)\n",
    "\n",
    "## 2:\n",
    "zeros = torch.zeros(3, 4)\n",
    "print(\"## 2: zeros\\n\", zeros)\n",
    "\n",
    "## 3:\n",
    "ones = torch.ones(2, 3)\n",
    "print(\"## 3: ones\\n\", ones)\n",
    "\n",
    "## 4:\n",
    "v = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "vzero = torch.ones_like(v)  # we might also use zeros_like\n",
    "print(\"## 4: zeros like\\n\", vzero)\n",
    "\n",
    "## 5:\n",
    "## torch.arange()\n",
    "# start 0, end 12 (exclusive)\n",
    "varange = torch.arange(0, 12).reshape(3, 4)\n",
    "## torch.linspace()\n",
    "# start 0, end 11 (inclusive), 12 steps\n",
    "vlinspace = torch.linspace(0, 11, 12).reshape(3, 4)\n",
    "print(\"## 5:\\narange\\n\", varange, \"\\nlinspace\\n\", vlinspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef1b8b",
   "metadata": {},
   "source": [
    "Notice that `arange` creates integer numbers, `linspace` floating point numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e58877dfd4627fc",
   "metadata": {},
   "source": [
    "### Random arrays\n",
    "\n",
    "As in NumPy, you have a big choice of random distributions to sample you arrays from.\n",
    "Try to do the same random arrays you tried to do in NumPy in Exercise 1:\n",
    "1) Create a random tensor of size 4 of uniform floating point numbers in the interval [0, 1). (see [`torch.rand`](https://pytorch.org/docs/stable/generated/torch.rand.html))\n",
    "2) Create a random tensor of size (3, 2) of uniform floating point numbers in the interval [0, 5). (hint: generate numbers in the interval [0, 1) and scale them up by 5).\n",
    "3) Create a random tensor of size (2, 1, 2) of integers in the interval [10, 20]. (see [`torch.randint`](https://pytorch.org/docs/stable/generated/torch.randint.html), caraful with border conditions!)\n",
    "4) Create a random tensor of size 10 over the normal distribution, mean 3 and std dev 2. (see [`torch.normal`](https://pytorch.org/docs/stable/generated/torch.normal.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "108b3718b4ff25d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T20:29:56.713417Z",
     "start_time": "2024-09-18T20:29:56.707816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1: rand\n",
      " tensor([0.8823, 0.9150, 0.3829, 0.9593])\n",
      "## 2: rand * 5\n",
      " tensor([[1.9522, 3.0045],\n",
      "        [1.2829, 3.9682],\n",
      "        [4.7039, 0.6659]])\n",
      "## 3: randint\n",
      " tensor([[[11, 20]],\n",
      "\n",
      "        [[13, 14]]])\n",
      "## 4: normal distribution\n",
      " tensor([3.7531, 2.6384, 3.7861, 3.8654, 0.2746, 5.7129, 4.3376, 1.5846, 2.3466,\n",
      "        2.4424])\n"
     ]
    }
   ],
   "source": [
    "## 1:\n",
    "rand = torch.rand(4)\n",
    "print(\"## 1: rand\\n\", rand)\n",
    "\n",
    "## 2: \n",
    "rand05 = torch.rand(3, 2) * 5\n",
    "print(\"## 2: rand * 5\\n\", rand05)\n",
    "\n",
    "## 3:\n",
    "# low inclusive, high exclusive\n",
    "randint = torch.randint(10, 21, (2, 1, 2))\n",
    "print(\"## 3: randint\\n\", randint)\n",
    "\n",
    "## 4:\n",
    "normal = torch.normal(mean=3., std=2., size=(10,))\n",
    "print(\"## 4: normal distribution\\n\", normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298efd452bfa973",
   "metadata": {},
   "source": [
    "## Device (GPU vs CPU)\n",
    "\n",
    "In this section we will learn how do computation using the GPU instead of the CPU: notice that this is the reason why in Deep Learning applications PyTorch is used over NumPy.\n",
    "\n",
    "By default, tensors are accessed by the CPU. You can check it easily using the [`.device()`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device) method.\n",
    "1) Create an identity matrix of size (4, 4) and access its device attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a205fdbd7b65c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T20:17:23.060931Z",
     "start_time": "2024-09-18T20:17:23.055769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1: device = cpu\n"
     ]
    }
   ],
   "source": [
    "## 1: see .device of a matrix\n",
    "\n",
    "v = torch.eye(4, 4)  # create a tensor\n",
    "print(\"## 1: device =\", v.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd04377c451cc9e",
   "metadata": {},
   "source": [
    "Hence, every time we want to use the GPU, we need to explicitly move the tensors to the desired device. Careful here: your laptop doesn't necessarily have a dedicated GPU. And, even if it has one, it might not be compatible with CUDA (the NVIDIA interface that allows computations to be performed on the GPU).\n",
    "\n",
    "You can check if CUDA is available on your machine by simply using [`cuda.is_available()`](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40633cabf32ec826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T20:08:57.159307Z",
     "start_time": "2024-09-18T20:08:57.154986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The output might be different on your machine. \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848fb3e3e1d87a3",
   "metadata": {},
   "source": [
    "If the above returns False, it could be either because you didn't install correctly CUDA, or because you laptop doesn't have a GPU compatible with it. \n",
    "If you have a macbook with Apple Silicon processors, you can still use the device `mps`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34009eeefef2b8a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T20:12:06.271423Z",
     "start_time": "2024-09-18T20:12:06.267589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For mac M1/2/3 users\n",
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578906289e3a356",
   "metadata": {},
   "source": [
    "We can set the device either to these three options:\n",
    "- `cuda` (if you have a NVIDIA graphics card). Might be `cuda:0` etc if you have more than one.\n",
    "- `mps` (if you have a MacBook with M1/2/3 processor)\n",
    "- `cpu` otherwise\n",
    "\n",
    "If your laptop doesn't have any of the above-mentioned devices apart from the CPU, you can use Google Colab's or Kaggle's notebooks: they offer free hours of GPU per week (they count the hours the kernel is running, not if you are actually using the notebook. Remember to shut it down when you don't use it!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc0d82333ea4030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T20:17:26.711323Z",
     "start_time": "2024-09-18T20:17:26.707268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7ceb325529382",
   "metadata": {},
   "source": [
    "Finally, move the tensor `v` you created earlier to the most convenient device. Use function [`.to`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html). Careful: is it an in-place method? Check that the device is indeed correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c331d995b0e7c26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T20:24:40.156801Z",
     "start_time": "2024-09-18T20:24:40.153872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w device = mps:0 , v device = cpu\n"
     ]
    }
   ],
   "source": [
    "# Move vector v to the correct device. Check it is indeed on the desired device.\n",
    "w = v.to(device)\n",
    "print(\"w device =\", w.device, \", v device =\", v.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319dd187",
   "metadata": {},
   "source": [
    "Notice that `w` is moved to the GPU. `v`, instead, remains on the CPU. `.to` is not an inplace method, it returns a new tensor allocated in the desired device. Notice that changing `w` doesn't affect `v`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7171999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w after change: tensor(10., device='mps:0') , v after change: tensor(1.) : they are different objects!\n"
     ]
    }
   ],
   "source": [
    "w[0, 0] = 10.\n",
    "print(\"w after change:\", w[0, 0], \", v after change:\", v[0, 0], \": they are different objects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9932e553cde60656",
   "metadata": {},
   "source": [
    "You can also create a tensor and send it directly to the correct device. \n",
    "1. Create a tensor of ones of size (3, 3) and specify in its constructor the `device` attribute. Check that, indeed, the tensor has been initialized with the correct device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f9247113e7f4981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T06:49:34.057350Z",
     "start_time": "2024-09-19T06:49:34.051445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor is allocated in: mps:0\n"
     ]
    }
   ],
   "source": [
    "## 1: Create a tensor and initialize it to the correct device.\n",
    "eye = torch.ones(3, device=device)\n",
    "print(\"The tensor is allocated in:\", eye.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad27eb333bb8200",
   "metadata": {},
   "source": [
    "Later on, we will compute empirically how much faster are GPUs than CPUs for computing large calculations.\n",
    "\n",
    "If you are using Kaggle platform for your projects (we recommend you to do that), you have at your disposal 30h/week of free GPUs: in order to activate it, you need to open a notebook, go to settings -> accelerator and you can select a GPU from there. If the GPU options are non-clickable, it is because you have to verify your account using your phone number. Go to home -> your picture (top right border) -> settings -> phone verification. Before the options become actually clickable you will need to wait a few minutes (<5').\n",
    "\n",
    "If you are using Google Colab (also recommended), you can activate GPUs by opening a notebook -> top-right arrow pointing downward -> change runtime type -> select something which is not `CPU`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d972460fb05e0a",
   "metadata": {},
   "source": [
    "## Working with tensors' dimensions\n",
    "\n",
    "In this section we will learn how to manipulate tensor's dimensions. Notice that they are extremely similar to NumPy methods: hence, if you have done exercise 1, this section should be quite straightforward.\n",
    "\n",
    "### Access elements and slicing \n",
    "\n",
    "Create an identity matrix of size (4, 4) and access \n",
    "1. The element in position [0, 0]\n",
    "2. The last element\n",
    "3. Element in position [2, 3]\n",
    "Check that the returned elements are what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60958f0e86811752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T06:54:09.454500Z",
     "start_time": "2024-09-19T06:54:09.451717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1: we expect 1, we get tensor(1.)\n",
      "## 2: we expect 1, we get tensor(1.)\n",
      "## 3: we expect 0, we get tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Create the identity matrix\n",
    "eye = torch.eye(4)\n",
    "\n",
    "## 1: access element in [0, 0]\n",
    "print(\"## 1: we expect 1, we get\", eye[0, 0])\n",
    "\n",
    "## 2: access element in [3, 3] , rember -1 is for last position\n",
    "print(\"## 2: we expect 1, we get\", eye[-1, -1])\n",
    "\n",
    "## 3: access element in [2, 3]\n",
    "print(\"## 3: we expect 0, we get\", eye[2, -3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e067adf3b30b97",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "1. Create a random tensor of size (3, 4) of integers in the interval [5, 10].\n",
    "2. Print the second row.\n",
    "3. Print the third column.\n",
    "4. Print the sub-matrix spanning from the second to the third row, from the second to the third column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dddd2e53d3e6ac70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1\n",
      " tensor([[10,  9, 10,  7],\n",
      "        [ 9,  5,  7,  5],\n",
      "        [ 6,  8,  8, 10]])\n",
      "## 2: second row tensor([9, 5, 7, 5])\n",
      "## 3: third column tensor([10,  7,  8])\n",
      "## 4: sub-matrix\n",
      " tensor([[5, 7],\n",
      "        [8, 8]])\n"
     ]
    }
   ],
   "source": [
    "## 1: create a random tensor of size [3, 4].\n",
    "v = torch.randint(5, 11, size=(3, 4))\n",
    "print(\"## 1\\n\", v)\n",
    "\n",
    "## 2: print the second row.\n",
    "print(\"## 2: second row\", v[1, :])  # second row has index 1\n",
    "\n",
    "## 3: print the third column.\n",
    "print(\"## 3: third column\", v[:, 2])  # Third column has index 2\n",
    "\n",
    "## 4: sub-matrix\n",
    "print(\"## 4: sub-matrix\\n\", v[1:3, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a439f",
   "metadata": {},
   "source": [
    "### Access tensors' dimensions\n",
    "\n",
    "1. Create a tensor $v$ of size (3, 4, 2, 4, 1) of random floats in [0, 1)\n",
    "2. Print its shape. You can use both `.shape` and `.size()`, try them both.\n",
    "3. Print its third dimension's size (2 in our example). Check `.size()` function.\n",
    "4. Print the number of dimensions of our vector (5 in our example). Check `.ndim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "602276de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 2: v.size() = torch.Size([3, 4, 2, 4, 1]) ; v.shape = torch.Size([3, 4, 2, 4, 1])\n",
      "Pam from Dunder Mifflin: they are the same picture;)\n",
      "## 3: third dimension, we expect 2, we get: 2\n",
      "We could also use .shape: 2\n",
      "## 4: number of dimensions: 5\n"
     ]
    }
   ],
   "source": [
    "## 1: create a random tensor v of size (3, 4, 2, 4, 1).\n",
    "v = torch.rand(3, 4, 2, 4, 1)\n",
    "\n",
    "## 2: print v's shape using .shape and .size().\n",
    "print(\"## 2: v.size() =\", v.size(), \"; v.shape =\", v.shape)\n",
    "print(\"Pam from Dunder Mifflin: they are the same picture;)\")\n",
    "\n",
    "## 3: print the size of the third dimension of v.\n",
    "# Index of the third dimension is 2!!\n",
    "print(\"## 3: third dimension, we expect 2, we get:\", v.size(2))\n",
    "print(\"We could also use .shape:\", v.shape[2])\n",
    "\n",
    "## 4: print the number of dimensions of v.\n",
    "print(\"## 4: number of dimensions:\", v.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3beec5d76fa615b",
   "metadata": {},
   "source": [
    "## Permute dimensions\n",
    "\n",
    "You can invert the order of the dimensions of a tensor. Create a random tensor of integers in the interval [0, 10) of size (2, 3, 4) and permute its dimensions so that the final size is (4, 2, 3). See [`torch.permute`](https://pytorch.org/docs/stable/generated/torch.permute.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1e286754f2b797f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T07:01:03.982170Z",
     "start_time": "2024-09-19T07:01:03.979816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([2, 3, 4])\n",
      "Shape of w after having permuted v's dimensions: torch.Size([4, 2, 3])\n",
      "Notice that v has remained unchanged: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Create a random tensor. Check its shape (2, 3, 4)\n",
    "v = torch.randint(0, 10, (2, 3, 4))\n",
    "print(\"Original shape:\", v.size())\n",
    "\n",
    "# Permute its dimensions. Check its shape (4, 2, 3)\n",
    "# first we want the third dimension, second the first dimension, and last the second dimension\n",
    "# (indices start from 0!)\n",
    "w = torch.permute(v, (2, 0, 1))\n",
    "print(\"Shape of w after having permuted v's dimensions:\", w.size())\n",
    "print(\"Notice that v has remained unchanged:\", v.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d684fd09d5a4bb8",
   "metadata": {},
   "source": [
    "## Squeeze/unsqueeze\n",
    "\n",
    "If you want increase the number of dimensions of your vector (similar to `np.newaxis`, this might turn useful in the context of broadcasting), you can use [`torch.unsqueeze`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html). If you want to reduce the number of dimensions of your vector by dropping dimensions of size 1 you can use [`torch.squeeze`](https://pytorch.org/docs/stable/generated/torch.squeeze.html) instead.\n",
    "\n",
    "1. Create a random tensor uniform in [0, 1) of size (2, 2). Insert a new dimension so that the final shape is (2, 1, 2)\n",
    "2. Add a dimension to the tensor of point 1, so that the final shape is (2, 1, 2, 1). Try to use negative indices as the argument of `torch.unsqueeze()`.\n",
    "3. Turn the tensor back to its original shape (2, 2) by using `torch.squeeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b671e74f22171ec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T07:15:03.127711Z",
     "start_time": "2024-09-19T07:15:03.124690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1: v's shape after having added a dimension: torch.Size([2, 1, 2])\n",
      "## 2: v's dimension after having added a new dimension as its last: torch.Size([2, 1, 2, 1])\n",
      "## 3: v back to normal: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "## 1: Create a tensor of size (2, 2). Unsqueeze it so that its final shape is (2, 1, 2)\n",
    "v = torch.rand(2, 2)\n",
    "# We insert a new dimension as second dimension (index 1)\n",
    "v = torch.unsqueeze(v, 1)\n",
    "print(\"## 1: v's shape after having added a dimension:\", v.shape)\n",
    "\n",
    "## 2: Add an additional dimension to the tensor so that its shape is (2, 1, 2, 1). Use negative indices\n",
    "v = torch.unsqueeze(v, -1)  # Add a new dimension as the last dimension.\n",
    "print(\"## 2: v's dimension after having added a new dimension as its last:\", v.size())\n",
    "\n",
    "## 3: Turn the tensor back to shape (2, 2)\n",
    "# Simply use squeeze() without arguments: it will drop all dimensions of size 1.\n",
    "v = torch.squeeze(v)\n",
    "print(\"## 3: v back to normal:\", v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93229b6fa5c1e07",
   "metadata": {},
   "source": [
    "## Concatenate and stack\n",
    "\n",
    "If you have two tensors of compatible sizes, you can merge them into a unique tensor along one of their axes.\n",
    "In order to get some intuition, think about having 2 2-dimensional tensors of size (3, 4). You can merge them along the first axis and get the final shape be (6, 4), or you can merge them along the second axis and get the final shape to be (3, 8), or you can go in 3D stacking one over the other (along the z-axis) and get a shape of (2, 3, 4). \n",
    "\n",
    "This is precisely what [`torch.concat`](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat) (also called `.cat`) and [`torch.stack`](https://pytorch.org/docs/stable/generated/torch.stack) do. \n",
    "You should already be familiar with NumPy `axis` attribute. In PyTorch it is called `dim`.\n",
    "\n",
    "1. Concat $v$ and $w$ along the first dimension. Check that the final shape is (6, 4).\n",
    "2. Concat $v$ and $w$ along the second dimension. Check that the final shape is (3, 8).\n",
    "3. Concat $v$ and $w$ along a new dimension. Check that the final shape is (2, 3, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9399001d3b3efa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:16:40.985443Z",
     "start_time": "2024-09-19T08:16:40.982553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1: concatenating along first dimension:\n",
      "tensor([[7, 3, 2, 9],\n",
      "        [1, 3, 9, 0],\n",
      "        [9, 5, 8, 2],\n",
      "        [3, 7, 7, 3],\n",
      "        [9, 7, 6, 9],\n",
      "        [7, 2, 0, 0]])\n",
      "The shape is: torch.Size([6, 4])\n",
      "## 2: concatenating along the second dimension:\n",
      "tensor([[7, 3, 2, 9, 3, 7, 7, 3],\n",
      "        [1, 3, 9, 0, 9, 7, 6, 9],\n",
      "        [9, 5, 8, 2, 7, 2, 0, 0]])\n",
      "The shape is: torch.Size([3, 8])\n",
      "## 3: concatenating along a new dimension:\n",
      "tensor([[[7, 3, 2, 9],\n",
      "         [1, 3, 9, 0],\n",
      "         [9, 5, 8, 2]],\n",
      "\n",
      "        [[3, 7, 7, 3],\n",
      "         [9, 7, 6, 9],\n",
      "         [7, 2, 0, 0]]])\n",
      "The shape is: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "v = torch.randint(0, 10, (3,4))\n",
    "w = torch.randint(0, 10, (3,4))\n",
    "\n",
    "## 1: concat along first dimension.\n",
    "# v, w need to be passed as a tuple/list of tensors.\n",
    "x = torch.concat((v, w), dim=0)\n",
    "print(\"## 1: concatenating along first dimension:\")\n",
    "print(x)\n",
    "print(\"The shape is:\", x.shape)\n",
    "\n",
    "## 2: concat along second dimension.\n",
    "y = torch.concat((v, w), dim=1)\n",
    "print(\"## 2: concatenating along the second dimension:\")\n",
    "print(y)\n",
    "print(\"The shape is:\", y.shape)\n",
    "\n",
    "## 3: concat along new dimension.\n",
    "# In order to add a new dimension we use stack\n",
    "z = torch.stack((v, w))\n",
    "print(\"## 3: concatenating along a new dimension:\")\n",
    "print(z)\n",
    "print(\"The shape is:\", z.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695c739ef39b4bb",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "Same as in NumPy, also PyTorch tensors allow [broadcasting](https://pytorch.org/docs/stable/notes/broadcasting.html).\n",
    "When performing element-wise operations (like sums) on two tensors of mismatching sizes, the smaller tensor can adapt to the size of the larger tensor in case these simple rules apply:\n",
    "\n",
    "- Each tensor has at least one dimension.\n",
    "- When iterating over the dimension sizes, starting at the trailing (right-most) dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
    "\n",
    "Let us see an example:\n",
    "\n",
    "Assume you have $v = [[1, 2, 3], [4, 5, 6]]$ shape (2, 3) and $w=[3, 2, 1]$ shape (3). If we want to perform $v + w$ (element by element sum), it is clear that the dimensions don't match, but with the help of broadcasting we can still do it: $w$ is simply enlarged to reach size (2, 3) by copying itself on the first axis twice. Then, it is possible to perform element by element sum $v+w$.\n",
    "\n",
    "Let's put broadcasting in practice:\n",
    "\n",
    "1. Perform the above described example $v+w$ using tensors, check that the result size is (2, 3) and that numbers add up.\n",
    "2. $r = [[1, 2], [3, 4], [5, 6]]$ and $l=[1, 2, 3]$. Compute $r + l$. It should raise errors. Why?\n",
    "3. Adjust the size of $l$ in example 2 so that the sum works. What size should $l$ have in order for broadcasting to work on $r + l$?\n",
    "4. Create random integers tensors $s$ of size (2, 1, 3, 1) and $t$ of size (1, 3, 1, 3). Does broadcasting work here in order to compute $s+t$? In case it does, predict the final shape of the result. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25c1ff851e016878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T08:58:04.389094Z",
     "start_time": "2024-09-19T08:58:04.387183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1: v + w\n",
      " tensor([[4, 4, 4],\n",
      "        [7, 7, 7]])\n",
      "## 2: broadcasting doesn't work, trailing dimensions don't match: 2 , 3\n",
      "## 3: use unsqueeze() to add a dimension to l:\n",
      " tensor([[2, 3],\n",
      "        [5, 6],\n",
      "        [8, 9]])\n",
      "## 4: the final shape should be (2, 3, 3, 3): and it is torch.Size([2, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "## 1: compute v + w\n",
    "v = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "w = torch.tensor([3, 2, 1])\n",
    "print(\"## 1: v + w\\n\", v + w)\n",
    "\n",
    "## 2: compute r + l. It doesn't work, why?\n",
    "r = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "l = torch.tensor([1, 2, 3])\n",
    "try:\n",
    "    r + l\n",
    "except RuntimeError:\n",
    "    print(\"## 2: broadcasting doesn't work, trailing dimensions don't match:\", r.shape[-1], \",\", l.shape[-1])\n",
    "\n",
    "## 3: adjust the size of l, and compute r + l\n",
    "# Since r has shape (3, 2) and l has shape (3),\n",
    "# We need to add an additional axis to l so that its shape is (3, 1).\n",
    "print(\"## 3: use unsqueeze() to add a dimension to l:\\n\", r + torch.unsqueeze(l, -1))\n",
    "\n",
    "## 4: compute s + t\n",
    "s = torch.rand(2, 1, 3, 1)\n",
    "t = torch.rand(1, 3, 1, 3)\n",
    "# yes, it works. proceeding right to left, \n",
    "# we fix dimensions with size 1 so that the two tensors' shapes coincide.\n",
    "print(\"## 4: the final shape should be (2, 3, 3, 3): and it is\", (s + t).shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ea76fcf50b553",
   "metadata": {},
   "source": [
    "## PyTorch functions\n",
    "\n",
    "In this section we are going to learn the basic functions of PyTorch.\n",
    "\n",
    "### Mean, min, max, sum ...\n",
    "\n",
    "These functions are quite self-explanatory, and they work the same way as in NumPy. The only detail we ought to pay attention to is the axis along we want to perform the function (in NumPy it was called `axis`, in PyTorch `dim`).\n",
    "\n",
    "Create a random tensor $v$ of ints of size (3, 2, 4) and print it.\n",
    "\n",
    "In order to be sure you have understood what is going on, always try to predict the result and then check that your prediction is wrong/correct.\n",
    "\n",
    "1. Compute the min value in the entire tensor.\n",
    "2. Compute the max value along axis 0.\n",
    "3. Compute the min along axis 1.\n",
    "4. Multi-dimensional axes: take the sum over axes (0, 1). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6212dd7b00b8bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[16, 99, 60, 99],\n",
      "         [75, 17, 60,  9]],\n",
      "\n",
      "        [[28, 25, 90,  7],\n",
      "         [40, 88, 79, 56]],\n",
      "\n",
      "        [[12, 37, 36, 38],\n",
      "         [58, 91, 12, 15]]])\n"
     ]
    }
   ],
   "source": [
    "# Create v of shape (3, 2, 4)\n",
    "v = torch.randint(0, 100, (3, 2, 4))\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de343d48896069b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T09:11:40.284320Z",
     "start_time": "2024-09-19T09:11:40.281917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1: min is  tensor(7)\n",
      "## 2: max along axis 0 has shape (3, 2):\n",
      " torch.return_types.max(\n",
      "values=tensor([[28, 99, 90, 99],\n",
      "        [75, 91, 79, 56]]),\n",
      "indices=tensor([[1, 0, 1, 0],\n",
      "        [0, 2, 1, 1]]))\n",
      "## 3: min along axis 1 has shape (3, 4):\n",
      " torch.return_types.min(\n",
      "values=tensor([[16, 17, 60,  9],\n",
      "        [28, 25, 79,  7],\n",
      "        [12, 37, 12, 15]]),\n",
      "indices=tensor([[0, 1, 0, 1],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 1, 1]]))\n",
      "## 3: sum over axes (0, 1). It means both axes 0 and 1 are squashed, the final size is only 4: tensor([229, 357, 337, 224])\n"
     ]
    }
   ],
   "source": [
    "## 1: Compute the min value of v.\n",
    "print(\"## 1: min is \", v.min())\n",
    "\n",
    "## 2: Compute the max along axis 0.\n",
    "# Notice that max() also tells you the 0-axis indices where the max happens.\n",
    "print(\"## 2: max along axis 0 has shape (3, 2):\\n\", v.max(dim=0))\n",
    "\n",
    "## 3: Compute the min along axis 1.\n",
    "print(\"## 3: min along axis 1 has shape (3, 4):\\n\", v.min(dim=1))\n",
    "\n",
    "## 4: Compute the sum over axes (0, 1)\n",
    "print(\"## 3: sum over axes (0, 1). \" +\n",
    "      \"It means both axes 0 and 1 are squashed, the final size is only 4:\", v.sum(dim=(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f8062d06c1ada",
   "metadata": {},
   "source": [
    "### dot, matmul, transpose, *\n",
    "\n",
    "Unlike NumPy, Torch has a stricter policy on these operands:\n",
    "\n",
    "- `*`: is the Hadamard product, element-wise product.\n",
    "- `dot`: only used to compute the dot product of two 1-dimensional tensors. Remember how confusing the dot product between multi-dimension NumPy vectors is (see Exercise 1)? PyTorch avoids this issue by simply forbidding the dimension of the input tensors to be greater than 1.\n",
    "- `matmul`: or its alias `@` computes the matrix product. Can be used for larger than 2-dimensional tensors (it applies broadcasting, as much as in NumPy). Notice that the complexity of multiplying two $n\\times n$ matrices is $O(n^3)$. We are taking advantage of its relatively high time-complexity in order to show how much faster are GPUs wrt CPUs.\n",
    "\n",
    "1. Create two random integer tensors $A$ and $B$ of compatible(?) sizes and compute their Hadamard product (element by element product). Try these sizes (predict whether they work or not):\n",
    "    - $A$ size (3, 4), $B$ size (3, 4).\n",
    "    - $A$ size (3, 4), $B$ size (4, 4).\n",
    "    - $A$ size (3, 4), $B$ size (1, 4).\n",
    "2. Create two random 1-dimensional tensors $v, w$ and compute their dot product. If you can use multiple ways to compute it, check that indeed they return the same value.\n",
    "3. Create $C$ of size (3, 4) and $D$ of size (4, 3). Compute the matrix product. Are the sizes compatible?\n",
    "4. Create $E$ of size (3, 3) and $F$ of size (4, 3). Compute the matrix product. Are the sizes compatible? If not, use the transpose operator to adjust the dimensions of one of the two matrices and compute the matrix product.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3d65a6ea3ebcf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1: when both A and B have the same size then the hadamard product works:\n",
      " torch.Size([3, 4])\n",
      "## 1: B gets enlarged by broadcasting have the same size then the hadamard product works:\n",
      " torch.Size([3, 4])\n",
      "## 2: dot product tensor(1.2744)\n",
      "alternatively with matmul: tensor(1.2744)\n",
      "alternatively with @: tensor(1.2744)\n",
      "## 3: C @ D:\n",
      " torch.Size([3, 3])\n",
      "## 4: E @ F.T: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "## 1: Create A, B and perform hadamard product\n",
    "# A (3,4), B (3,4)\n",
    "A = torch.rand(3, 4)\n",
    "B = torch.rand(3, 4)\n",
    "print(\"## 1: when both A and B have the same size then the hadamard product works:\\n\", (A * B).shape)\n",
    "\n",
    "# A (3,4), B (4,4)\n",
    "# They have mismatching sizes!! Broadcasting cannot work either!\n",
    "\n",
    "# A (3,4), B (1,4)\n",
    "# Although they don't have the same size, broadcasting enlarges B to get to size (3, 4)\n",
    "A = torch.rand(3, 4)\n",
    "B = torch.rand(1, 4)\n",
    "print(\"## 1: B gets enlarged by broadcasting have the same size then the hadamard product works:\\n\", (A * B).shape)\n",
    "\n",
    "## 2: Create 1-dimensional tensors v, w and compute their dot product.\n",
    "v = torch.rand(3)\n",
    "w = torch.rand(3)\n",
    "print(\"## 2: dot product\", v.dot(w))\n",
    "print(\"alternatively with matmul:\", v.matmul(w))\n",
    "print(\"alternatively with @:\", v @ w)\n",
    "\n",
    "## 3: Compute matrix product of C and D.\n",
    "C = torch.rand(3, 4)\n",
    "D = torch.rand(4, 3)\n",
    "# Sizes are compatible (last of C = first of D)\n",
    "print(\"## 3: C @ D:\\n\", (C @ D).shape)\n",
    "\n",
    "## 4: adjust dimensions using .T, and compute the matrix product E @ F\n",
    "E = torch.rand(3, 3)\n",
    "F = torch.rand(4, 3)\n",
    "# Sizes are not compatible. We can transpose F so that the size is (3, 4)\n",
    "print(\"## 4: E @ F.T:\", (E @ F.T).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52ff8888dba981",
   "metadata": {},
   "source": [
    "Now, we try to prove empirically that GPUs are actually faster than CPUs at doing large calculations.\n",
    "\n",
    "Create large tensors $G, H$ both of size (15000, 15000). Take their matrix product and measure how long it takes (use `%%time` cell magic notebook function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "941d22401d6f340c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T09:49:37.404396Z",
     "start_time": "2024-09-19T09:49:35.677903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create E and F\n",
    "G = torch.rand(15000, 15000)\n",
    "H = torch.rand(15000, 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50f9bad11da746e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T09:49:55.733265Z",
     "start_time": "2024-09-19T09:49:45.723855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 2.23 s, total: 1min 39s\n",
      "Wall time: 9.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3747.3000, 3708.0327, 3715.6504,  ..., 3745.9561, 3738.2327,\n",
       "         3755.8008],\n",
       "        [3786.8828, 3728.3523, 3704.2632,  ..., 3764.4802, 3742.7209,\n",
       "         3758.7354],\n",
       "        [3792.3147, 3735.1011, 3735.4006,  ..., 3760.5247, 3771.2302,\n",
       "         3780.5728],\n",
       "        ...,\n",
       "        [3792.9155, 3742.2104, 3725.9783,  ..., 3759.2773, 3745.9236,\n",
       "         3777.8081],\n",
       "        [3777.2478, 3744.6997, 3736.3315,  ..., 3783.3958, 3738.6348,\n",
       "         3775.3091],\n",
       "        [3783.7690, 3756.2952, 3728.4785,  ..., 3748.6155, 3739.5891,\n",
       "         3749.5991]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "G @ H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53eec9dfd6c023b",
   "metadata": {},
   "source": [
    "Move $E$ and $F$ to the more convenient device at your disposal (different from CPU, if possible), and compute the same matrix product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "727c2e81c3e93874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T09:50:02.144090Z",
     "start_time": "2024-09-19T09:50:01.448426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Move the tensors to GPU in another cell, so that the time is not counted.\n",
    "G = G.to(device)\n",
    "H = H.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac5312cb12b5f605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T09:50:05.795896Z",
     "start_time": "2024-09-19T09:50:04.017497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.09 ms, sys: 7.59 ms, total: 14.7 ms\n",
      "Wall time: 18.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3747.3137, 3708.0364, 3715.6570,  ..., 3745.9404, 3738.2397,\n",
       "         3755.8044],\n",
       "        [3786.8901, 3728.3479, 3704.2625,  ..., 3764.4812, 3742.7344,\n",
       "         3758.7249],\n",
       "        [3792.3008, 3735.1030, 3735.3962,  ..., 3760.5261, 3771.2334,\n",
       "         3780.5669],\n",
       "        ...,\n",
       "        [3792.9199, 3742.2112, 3725.9788,  ..., 3759.2810, 3745.9275,\n",
       "         3777.8079],\n",
       "        [3777.2510, 3744.6982, 3736.3340,  ..., 3783.3857, 3738.6384,\n",
       "         3775.3181],\n",
       "        [3783.7734, 3756.2986, 3728.4778,  ..., 3748.6125, 3739.5864,\n",
       "         3749.5984]], device='mps:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "G @ H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f563c9873d5c",
   "metadata": {},
   "source": [
    "Side note: on my laptop (MacBook) I noticed a performance improvement by $\\approx\\times 10$. On Kaggle the performance improvement is much larger (from >20'' to <<1').\n",
    "When you have done this task, you might want to shut down your notebook and start from the cells below since resource usage might be quite demanding. Also, if you are using Kaggle, you might consider shutting down the GPU, since the bottom cells can be done with CPU only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf73d907408b34",
   "metadata": {},
   "source": [
    "### PyTorch functionals\n",
    "\n",
    "As you will learn by attending this class, one of the key features of neural networks are their non-linear functions.\n",
    "PyTorch has already implemented a great amount of them in the package `torch.nn.functionals`.\n",
    "Create a random tensor $A$ of size (3, 2) and apply to it:\n",
    "\n",
    "1. [ReLu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html)\n",
    "2. [Tanh](https://pytorch.org/docs/stable/generated/torch.nn.functional.tanh.html)\n",
    "3. [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html)\n",
    "4. [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) (it requires an axis: pick axis 1, predict the output shape).\n",
    "\n",
    "If you are not familiar with them don't worry, you will learn in the remainder of the course what these functions are used for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "893065c65c067750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T10:03:51.446781Z",
     "start_time": "2024-09-19T10:03:51.439223Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96717c9a8f603bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T10:03:53.475395Z",
     "start_time": "2024-09-19T10:03:53.472933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.7276,  1.4128],\n",
      "        [-0.1018,  3.6263],\n",
      "        [ 1.6128,  4.2273]])\n",
      "## 1: ReLU. Notice that negative numbers are set to 0.\n",
      " tensor([[0.0000, 1.4128],\n",
      "        [0.0000, 3.6263],\n",
      "        [1.6128, 4.2273]])\n",
      "## 2: tanh. Notice that numbers are in the interval (-1, 1)\n",
      " tensor([[-0.9998,  0.8881],\n",
      "        [-0.1015,  0.9986],\n",
      "        [ 0.9236,  0.9996]])\n",
      "## 3: sigmoid. Notice that numbers are in the interval (0, 1)\n",
      " tensor([[0.0088, 0.8042],\n",
      "        [0.4746, 0.9741],\n",
      "        [0.8338, 0.9856]])\n",
      "## 4: softmax, notice that along axis 1 numbers sum to 1. The shape is still (3, 2).\n",
      " tensor([[0.0021, 0.9979],\n",
      "        [0.0235, 0.9765],\n",
      "        [0.0682, 0.9318]])\n"
     ]
    }
   ],
   "source": [
    "# Create A in the interval (-5, 5)\n",
    "A = torch.rand(3, 2) * 10 - 5\n",
    "print(A)\n",
    "\n",
    "## 1: apply F.relu\n",
    "print(\"## 1: ReLU. Notice that negative numbers are set to 0.\\n\", F.relu(A))\n",
    "\n",
    "## 2: apply F.tanh\n",
    "print(\"## 2: tanh. Notice that numbers are in the interval (-1, 1)\\n\", F.tanh(A))\n",
    "\n",
    "## 3: apply F.sigmoid\n",
    "print(\"## 3: sigmoid. Notice that numbers are in the interval (0, 1)\\n\", F.sigmoid(A))\n",
    "\n",
    "## 4: apply F.softmax with dim=1\n",
    "print(\"## 4: softmax, notice that along axis 1 numbers sum to 1. The shape is still (3, 2).\\n\", F.softmax(A, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9176dfb298f4b9e",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "One of the useful features of PyTorch is that it is possible to compute automatically the gradient of functions. \n",
    "As you will see, the gradient of a function is one of the key ingredients of the backpropagation algorithm, used to train neural nets.\n",
    "\n",
    "Assume we have tensor $x = [2], y = [2]$. We have $z = 2x^2 + 3y = [14]$.\n",
    "\n",
    "We know that $\\frac{\\delta z}{\\delta x} = 4x$, $\\frac{\\delta z}{\\delta y} = 3$. Since we are evaluating the point $x=2, y=2$, we get that the gradient is (8, 3). The gradients are going to be stored in $x.grad$ and $y.grad$ if we specify the option `requires_grad=True`. We can let PyTorch compute the gradients by invoking `z.backward()`. Check that indeed `x.grad` and `y.grad` hold the desired values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e3e254da08cb300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T10:25:06.967129Z",
     "start_time": "2024-09-19T10:25:06.961072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.], dtype=torch.float64)\n",
      "tensor([3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2], dtype=torch.float64, requires_grad=True)\n",
    "y = torch.tensor([2], dtype=torch.float64, requires_grad=True)\n",
    "z = 2 * x*x + 3 * y\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f3648bc606d705",
   "metadata": {},
   "source": [
    "1. Create tensors $s = [1]$ and $t = [1]$, define a new variable $w = 5s + 6$ and compute their gradient. What is the gradient associated to $t$? (Notice that $w$ does not depend on $t$). \n",
    "2. What happens if I try to define an integer tensor with `requires_grad=True`?\n",
    "3. What happens if I call `numpy()` on a tensor that has `requires_grad=True`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54c046fde0ee306e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T12:03:47.093944Z",
     "start_time": "2024-09-19T12:03:47.091851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1: the gradient wrt t is: None , whereas the gradient for s is: tensor([5.])\n",
      "## 2: only floating point tensors can have require_gradients=True!\n",
      "## 3: only tensors without gradients can be turned to NumPy!\n"
     ]
    }
   ],
   "source": [
    "s = torch.tensor([1.], requires_grad=True)\n",
    "t = torch.tensor([1.], requires_grad=True)\n",
    "w = 5 * s + 6\n",
    "w.backward()\n",
    "\n",
    "## 1: gradient of t for w = 5s + 6.\n",
    "print(\"## 1: the gradient wrt t is:\", t.grad, \", whereas the gradient for s is:\", s.grad)\n",
    "\n",
    "## 2: integer tensor with requires_grad.\n",
    "try:\n",
    "    t = torch.tensor([1], requires_grad=True)\n",
    "except RuntimeError:\n",
    "    print(\"## 2: only floating point tensors can have require_gradients=True!\")\n",
    "\n",
    "## 3: compute .numpy() of a tensor with requires_grad.\n",
    "try:\n",
    "    s_numpy = s.numpy()\n",
    "except RuntimeError:\n",
    "    print(\"## 3: only tensors without gradients can be turned to NumPy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834bc4e",
   "metadata": {},
   "source": [
    "In this last section we point out a very important feature of gradients, namely that they are *cumulative*! In order to see what does that mean, let's see in practice the example that was given in class:\n",
    "\n",
    "1. Create tensors $x=[2], y=[3]$ (with flag `requires_grad=True`).\n",
    "2. Compute $z = x * x + y$ and perform the backward pass.\n",
    "3. Check that the gradients are as expected: $\\frac{\\delta z}{\\delta x}=2x=4$, $\\frac{\\delta z}{\\delta y} = 1$.\n",
    "4. Compute $g = xy + 3x$ and perform che backward pass.\n",
    "5. Check out the gradients: $\\frac{\\delta g}{\\delta x}=y + 3=6$, $\\frac{\\delta g}{\\delta y} = x = 2$.\n",
    "6. Do you see the expected value? Can you explain why? (hint: gradients are *cumulative*).\n",
    "7. In order to fix this potential issue, use `x/y.grad.zero_()` in between the computation of $z$ and $g$. Do you observe the expected gradient now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0e33d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 3: grad of x: tensor([4.]) , grad of y: tensor([1.])\n",
      "## 5: grad of x: tensor([10.]) , grad of y: tensor([3.])\n",
      "Gradients are cumulative, hence x.grad = 4 + 6 = 10, y.grad = 1 + 2 = 3.\n",
      "## 7: grad of x: tensor([6.]) , grad of y: tensor([2.])\n",
      "Now we see the expected gradient!\n"
     ]
    }
   ],
   "source": [
    "## 1: create x, y.\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "y = torch.tensor([3.], requires_grad=True)\n",
    "\n",
    "## 2: compute z.\n",
    "z = x * x + y\n",
    "z.backward()\n",
    "\n",
    "## 3: check out gradients of x, y.\n",
    "print(\"## 3: grad of x:\", x.grad, \", grad of y:\", y.grad)\n",
    "\n",
    "## 4: compute g.\n",
    "g = x * y + 3 * x\n",
    "g.backward()\n",
    "\n",
    "## 5: check out gradients of x, y\n",
    "print(\"## 5: grad of x:\", x.grad, \", grad of y:\", y.grad)\n",
    "print(\"Gradients are cumulative, hence x.grad = 4 + 6 = 10, y.grad = 1 + 2 = 3.\")\n",
    "\n",
    "## 7: Repeat 1-5 using torch.zero_grad()\n",
    "# We'll repeat everything with zero grad in between z and g:\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "y = torch.tensor([3.], requires_grad=True)\n",
    "z = x * x + y\n",
    "z.backward()\n",
    "# zero_grad()\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "g = x * y + 3 * x\n",
    "g.backward()\n",
    "print(\"## 7: grad of x:\", x.grad, \", grad of y:\", y.grad)\n",
    "print(\"Now we see the expected gradient!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
